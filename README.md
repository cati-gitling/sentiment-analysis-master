# Sentiment Analysis of Tweets by Donald Trump and Hillary Clinton:  
A Corpus Linguistic Study  
*Completed in mid-2021 as part of my Master's thesis at Technische Universität Berlin*

## Project Overview  
This repository documents the data cleaning, sentiment analysis, and exploratory data analysis conducted on political Twitter data using Python.

## Technologies and Methods Used  
Mainly Python tools and NLP frameworks such as NLTK and spaCy for sentiment analysis and data visualization.

## Objective  
The project demonstrates my ability to apply complex NLP techniques to analyze real-world datasets and document the process clearly.

## Additional Methods  
Beyond the Python-based analyses, further corpus linguistic methods—such as part-of-speech (POS) tagging and other linguistic analyses—were applied using tools outside the Python environment. This multi-methodological approach provides a comprehensive linguistic perspective.

## Datasets  
This project includes Twitter data from three sources:

- Tweets by **Donald Trump (@realDonaldTrump)**  
- Tweets by **Hillary Clinton (@HillaryClinton)**  
- Tweets by the official **New York Times (@nytimes)** Twitter account, used as a reference corpus

These datasets allow for comparison of political sentiment and discourse patterns across different political actors and a major media outlet.

## Usage  
Open the Jupyter Notebooks to follow the data cleaning, sentiment analysis, and exploratory steps. A rendered HTML version of the notebooks is also provided for easy viewing.

## Additional Scripts  
The repository includes the Python script `get_tweets.py` for accessing the Twitter API using Tweepy. This script was originally taken from the user GitLaura and adapted to meet the requirements of my master's thesis. It was used to download the tweets analyzed in this project.

This script demonstrates my skills in working with APIs, data acquisition, and modifying existing code to fit project needs.

## Data Organization and Usage  

### Recommended file structure  
- `/notebooks/` – Jupyter Notebook files (`.ipynb`) and their rendered HTML versions.  
- `/scripts/` – Python scripts, including `get_tweets.py` for Twitter API access.  
- `/data/` – (Optional) CSV files with raw tweet data used in the analyses.

### About the CSV files  
- If CSV files generated by the API script are manageable in size and legally shareable, they can be included in `/data/` for immediate use.  
- If CSV files are large or data access is straightforward via the Twitter API, it is recommended to exclude CSVs and instead run `get_tweets.py` to fetch fresh data.  
- Consult the Python script in `/scripts/` to download and prepare datasets.  
- Ensure `.gitignore` excludes local or temporary files from the repository.

### Reproducing the analysis  
1. Use existing CSV files in `/data/` (if provided) or run `get_tweets.py` to download fresh data.  
2. Open the Jupyter Notebooks in `/notebooks/` to follow the full analysis workflow.  
3. Refer to this README and inline comments for guidance.

---

If you have any questions about the project or file structure, feel free to contact me at [github.com/cati-gitling](https://github.com/cati-gitling).
